{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert GitHub Actions `.yaml` to Parallel Works ACTIVATE `.yaml`\n",
    "\n",
    "ACTIVATE `.yaml`-based workflows have the same \"feel\" as GitHub Actions so there are many similarities. However, they are not the same. GitHub Actions has a much wider feature set and `.yaml`s that run on GitHub runners have the advantage of implicit context with many additional environment variables.\n",
    "\n",
    "This notebook is a first draft, interactive tool for ingesting a GitHub Actions `.yaml`, clearing out some things that Parallel Works ACTIVATE definitely does not support, adding some boilerplate \"header\" information that is likely to be useful for ACTIVATE users, and keeping key workflow steps that ACTIVATE does support (namely the `jobs: steps: run:` framework and any explicit `env:` variables).\n",
    "\n",
    "This notebook is a work in progress and not a complete solution hence it is a living document and meant for experimentation. Any `.yaml` files generated here will probably require some kind of manual adjustment to make them actually runnable on ACTIVATE. However, the goal is for the bulk of the work of converting from GitHub `.yaml` to ACTIVATE `.yaml` can be achieved here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Input and output files, dependencies, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ~ does not always work, so use full path\n",
    "input_file=\"/home/jovyan/work/ubuntu-ci-x86_64-gnu.yaml\"\n",
    "output_file=input_file+\".2pw.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dependencies may be already installed if running this notebook\n",
    "# within a JupyterLab instance.\n",
    "#!pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from yaml.resolver import BaseResolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================\n",
    "# Additional set up so that | is included in the output.\n",
    "#==========================================================\n",
    "# According to flyx on 04/13/2021 at https://stackoverflow.com/questions/67080308/how-do-i-add-a-pipe-the-vertical-bar-into-a-yaml-file-from-python\n",
    "# to get the |, you need to create a new \"flag\" string (the AsLiteral)\n",
    "# which is then associated with the | by represent_literal_str.\n",
    "# All blocks that you want a | in need to be passed to AsLiteral.\n",
    "# Content in this cell is distributed under CC BY-SA 4.0, more\n",
    "# information at: https://creativecommons.org/licenses/by-sa/4.0/\n",
    "\n",
    "# Define a custom string subclass.\n",
    "# This is a marker to tell the representer which objects to handle.\n",
    "class AsLiteral(str):\n",
    "    pass\n",
    "\n",
    "# Define a custom representer function for the new class.\n",
    "# It tells the dumper to represent this type of object as a literal scalar.\n",
    "def represent_literal_str(dumper, data):\n",
    "    return dumper.represent_scalar(BaseResolver.DEFAULT_SCALAR_TAG, data, style=\"|\")\n",
    "\n",
    "# Register the representer function with the Dumper.\n",
    "# This makes it available for both the standard `yaml.dump` and `yaml.safe_dump`.\n",
    "yaml.add_representer(AsLiteral, represent_literal_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the file you want to convert\n",
    "with open(input_file) as stream:\n",
    "    try:\n",
    "        data = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', True, 'concurrency', 'defaults', 'env', 'jobs'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop things ACTIVATE `.yaml` does not support and keep `env:` and `jobs:`\n",
    "\n",
    "Some key things that ACTIVATE `.yaml` does support:\n",
    "+ `env:`\n",
    "+ `jobs:`\n",
    "+ `ssh:` - Unique to ACTIVATE `.yaml` for selecting a resource to run on\n",
    "\n",
    "ACTIVATE `.yaml` has an `on:` section. When `pyyaml` loads the `.yaml`, since `on:` is empty, it gives it the `True` dictionary entry. Since ACTIVATE `on:` is different from GitHub `on:`, we just remove the original GitHub `on:` section and replace it with a section relevant to ACTIVATE, below. ACTIVATE's `on:` section is used to pass input parameters to the workflow. It does not neccessarily need to be at the end of the `.yaml` - it can be at the beginning or end. Most examples built into ACTIVATE have `on:` at the end of the file so users can focus on the `steps:` of the workflow so that convention is followed here.\n",
    "\n",
    "Another important automated change is that `runs-on:` is removed and replaced with `ssh:` for use with the automated detection of connected resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['env', 'jobs'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of som GitHub Actions things that ACTIVATE does not use\n",
    "data.pop('name')\n",
    "data.pop(True)\n",
    "data.pop('concurrency')\n",
    "data.pop('defaults')\n",
    "\n",
    "# Print remaining keys\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting step index: 0\n"
     ]
    }
   ],
   "source": [
    "# Change runs-on: to ssh:\n",
    "# Get list of jobs, header_items first, before iterating\n",
    "# because otherwise Python throws an error when size\n",
    "# of the dictionary changes during the iteration.\n",
    "\n",
    "jobs_list = list(data['jobs'].keys())\n",
    "for job in jobs_list:\n",
    "\n",
    "    header_list = list(data['jobs'][job].keys())\n",
    "    for header_item in header_list:\n",
    "        \n",
    "        # Deal with runs-on:\n",
    "        if header_item == \"runs-on\":\n",
    "            # Create a new entry for ssh:\n",
    "            data['jobs'][job][\"ssh\"] = data['jobs'][job][\"runs-on\"]\n",
    "            # Populate ssh: parameters (need to add ACTIVATE inputs, see below)\n",
    "            data['jobs'][job][\"ssh\"] = {\"remoteHost\": \"${{inputs.resource.ip}}\"}\n",
    "            # Get rid of runs-on:\n",
    "            del data['jobs'][job][\"runs-on\"]\n",
    "        \n",
    "        # Each step needs to be reprocessed with |\n",
    "        if header_item == \"steps\":\n",
    "            \n",
    "            steps_list = list(data['jobs'][job][\"steps\"])\n",
    "            steps_to_delete = []\n",
    "            for ss, step in enumerate(steps_list):\n",
    "                if step['name'] == \"checkout\":\n",
    "                    # Special case likely with uses: actions/checkout\n",
    "                    # We could just delete it...\n",
    "                    #steps_to_delete.append(ss)\n",
    "                    # ...but instead of that, let's reset the content\n",
    "                    # to match some workflow inputs\n",
    "                    data['jobs'][job][\"steps\"][ss]['run'] = AsLiteral(\n",
    "                        \"echo Cloning repo to $PWD\\ngit clone https://github.com/${{ inputs.gh_org }}/${{ inputs.gh_repo }}\\ncd ${{ inputs.gh_repo }}\\ngit checkout ${{ inputs.gh_branch }}\\n\")\n",
    "                    del data['jobs'][job]['steps'][ss]['uses']\n",
    "                    del data['jobs'][job]['steps'][ss]['with']\n",
    "                elif step['name'] == \"cleanup\":\n",
    "                    # Sometimes a blanket clear all on the runner\n",
    "                    steps_to_delete.append(ss)\n",
    "                else:\n",
    "                    # Flag all steps that we want to keep AsLiteral\n",
    "                    # so they recieve a | for human readable YAML.\n",
    "                    run_content = AsLiteral(step['run'])\n",
    "                    data['jobs'][job][\"steps\"][ss]['run'] = run_content\n",
    "                    \n",
    "            # Done looping over the steps, need to delete the steps\n",
    "            # marked for deletion\n",
    "            for ss in steps_to_delete:\n",
    "                print(\"Deleting step index: \"+str(ss))\n",
    "                del data['jobs'][job][\"steps\"][ss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append `on:` and workflow `inputs:`\n",
    "\n",
    "As mentioned briefly above, ACTIVATE `.yaml` uses the `on:` section to pass input parameters to a workflow. Here, we explicitly add the `on:` section and provide a template for some inputs that may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add on: and populate it with execute:\n",
    "# Notes:\n",
    "#------------\n",
    "# 1) The section that is created is actually `'on':` (quoted on:). ACTIVATE\n",
    "# default workflows have this quoted on: as well so it should\n",
    "# still work.\n",
    "#-----------\n",
    "# 2) Python bools (title case) are automatically\n",
    "# converted to YAML bools (all lower case).\n",
    "#-----------\n",
    "# 3) The resource input created here matches explicity with\n",
    "# the remoteHost: ${{inputs.resource.ip}} entry in the \n",
    "# ssh: block, above.\n",
    "#-----------\n",
    "# 4) Added \n",
    "data[\"on\"] = {\"execute\": \n",
    "                 {\"inputs\": \n",
    "                     {\"resource\":\n",
    "                         {\"label\": \"Workflow Target\",\n",
    "                          \"type\": \"compute-clusters\",\n",
    "                          \"autoselect\": True,\n",
    "                          \"optional\": False},\n",
    "                      \"gh_org\":\n",
    "                         {\"label\": \"GitHub organization/owner\",\n",
    "                          \"type\": \"string\",\n",
    "                          \"default\": \"parallelworks\"},\n",
    "                      \"gh_repo\":\n",
    "                         {\"label\": \"GitHub repository\",\n",
    "                          \"type\": \"string\",\n",
    "                          \"default\": \"spack-stack\"},\n",
    "                      \"gh_branch\":\n",
    "                         {\"label\": \"GitHub branch\",\n",
    "                          \"type\": \"string\",\n",
    "                          \"default\": \"canary\"}\n",
    "                     }\n",
    "                 }\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env:\n",
      "  BUILD_CACHE_PATH: /home/ubuntu/spack-stack/build-cache-new-spack-v1\n",
      "  SOURCE_CACHE_PATH: /home/ubuntu/spack-stack/source-cache\n",
      "jobs:\n",
      "  ubuntu-ci-c6a-x86_64-gnu-build:\n",
      "    steps:\n",
      "    - name: checkout\n",
      "      run: |\n",
      "        echo Cloning repo to $PWD\n",
      "        git clone https://github.com/${{ inputs.gh_org}}/${{ inputs.gh_repo}}\n",
      "        cd ${{ inputs.gh_repo }}\n",
      "        git checkout ${{ inputs.gh_branch }}\n",
      "    - name: prepare-directories\n",
      "      run: |\n",
      "        # DH* REVERT ME AFTER MERGE\n",
      "        mkdir -p ${BUILD_CACHE_PATH}\n",
      "        mkdir -p ${SOURCE_CACHE_PATH}\n",
      "    - name: create-buildcache\n",
      "      run: |\n",
      "        # Get day of week to decide whether to use build caches or not\n",
      "        DOW=$(date +%u)\n",
      "        # Monday is 1 ... Sunday is 7\n",
      "        if [[ $DOW == 7 ]]; then\n",
      "          export USE_BINARY_CACHE=false\n",
      "          echo \"Ignore existing binary cache for creating buildcache environment\"\n",
      "        else\n",
      "          export USE_BINARY_CACHE=true\n",
      "          echo \"Use existing binary cache for creating buildcache environment\"\n",
      "        fi\n",
      "\n",
      "        # Set up spack-stack\n",
      "        source ./setup.sh\n",
      "\n",
      "        declare -a TEMPLATES=(\"unified-dev\" \"skylab-dev\" \"cylc-dev\" \"neptune-dev\")\n",
      "        for TEMPLATE in \"${TEMPLATES[@]}\"; do\n",
      "          if [[ \"${TEMPLATE}\" == *\"unified-dev\"* ]]; then\n",
      "            export ENVNAME=ue-gcc-11.4.0-buildcache\n",
      "          elif [[ \"${TEMPLATE}\" == *\"skylab-dev\"* ]]; then\n",
      "            export ENVNAME=se-gcc-11.4.0-buildcache\n",
      "          elif [[ \"${TEMPLATE}\" == *\"cylc-dev\"* ]]; then\n",
      "            export ENVNAME=ce-gcc-11.4.0-buildcache\n",
      "          elif [[ \"${TEMPLATE}\" == *\"neptune-dev\"* ]]; then\n",
      "            export ENVNAME=ne-gcc-11.4.0-buildcache\n",
      "          fi\n",
      "          echo \"Creating environment ${ENVNAME} from template ${TEMPLATE}\"\n",
      "\n",
      "          export ENVDIR=$PWD/envs/${ENVNAME}\n",
      "          spack stack create env --site linux.default --template ${TEMPLATE} --name ${ENVNAME} --compiler gcc\n",
      "          spack env activate ${ENVDIR}\n",
      "\n",
      "          unset SPACK_DISABLE_LOCAL_CONFIG\n",
      "          export SPACK_SYSTEM_CONFIG_PATH=\"${ENVDIR}/site\"\n",
      "\n",
      "          # Find external packages\n",
      "          spack external find --scope system \\\n",
      "              --exclude bison --exclude openssl \\\n",
      "              --exclude curl --exclude python \\\n",
      "              --exclude meson\n",
      "          spack external find --scope system grep\n",
      "          spack external find --scope system sed\n",
      "          spack external find --scope system perl\n",
      "          spack external find --scope system wget\n",
      "          spack external find --scope system texlive\n",
      "          spack external find --scope system mysql\n",
      "\n",
      "          # Find compilers\n",
      "          spack compiler find --scope system\n",
      "\n",
      "          export SPACK_DISABLE_LOCAL_CONFIG=true\n",
      "          unset SPACK_SYSTEM_CONFIG_PATH\n",
      "\n",
      "          # For buildcaches\n",
      "          spack config add config:install_tree:padded_length:200\n",
      "\n",
      "          # Set compiler and MPI specs\n",
      "          spack config add \"packages:mpi:require:['openmpi@5.0.8']\"\n",
      "          spack config add \"packages:all:prefer:['%gcc']\"\n",
      "\n",
      "          # Add additional variants for MET packages, different from config/common/packages.yaml\n",
      "          spack config add \"packages:met:variants:+python +grib2 +graphics +lidar2nc +modis\"\n",
      "\n",
      "          # Concretize and check for duplicates\n",
      "          spack concretize --force --fresh 2>&1 | tee log.concretize.${ENVNAME}\n",
      "          ${SPACK_STACK_DIR}/util/show_duplicate_packages.py -i fms -i crtm -i crtm-fix -i esmf -i mapl -i py-cython -i neptune-env\n",
      "\n",
      "          # Add and update source cache\n",
      "          spack mirror add local-source file://${SOURCE_CACHE_PATH}/\n",
      "          spack mirror create -a -d ${SOURCE_CACHE_PATH}/\n",
      "\n",
      "          # Add binary cache if requested\n",
      "          if [ \"$USE_BINARY_CACHE\" = true ] ; then\n",
      "            set +e\n",
      "            spack mirror add local-binary file://${BUILD_CACHE_PATH}/\n",
      "            spack buildcache update-index local-binary\n",
      "            set -e\n",
      "            echo \"Packages in spack binary cache:\"\n",
      "            spack buildcache list\n",
      "          fi\n",
      "\n",
      "          # Break installation up in pieces and create build caches in between\n",
      "          # This allows us to \"spin up\" builds that altogether take longer than\n",
      "          # six hours, and/or fail later in the build process.\n",
      "\n",
      "          if [[ \"${TEMPLATE}\" == *\"unified-dev\"* || \"${TEMPLATE}\" == *\"skylab-dev\"* ]]; then\n",
      "            # base-env\n",
      "            echo \"base-env ...\"\n",
      "            spack install --fail-fast --source --no-check-signature base-env 2>&1 | tee log.install.${ENVNAME}.base-env\n",
      "            spack buildcache create -u local-binary base-env\n",
      "            # jedi-base-env\n",
      "            echo \"jedi-base-env ...\"\n",
      "            spack install --fail-fast --source --no-check-signature jedi-base-env 2>&1 | tee log.install.${ENVNAME}.jedi-base-env\n",
      "            spack buildcache create -u local-binary jedi-base-env\n",
      "            # jedi-ufs-env\n",
      "            echo \"jedi-ufs-env ...\"\n",
      "            spack install --fail-fast --source --no-check-signature jedi-ufs-env 2>&1 | tee log.install.${ENVNAME}.jedi-ufs-env\n",
      "            spack buildcache create -u local-binary jedi-ufs-env\n",
      "          elif [[ \"${TEMPLATE}\" == *\"neptune-dev\"* ]]; then\n",
      "            # base-env\n",
      "            echo \"base-env ...\"\n",
      "            spack install --fail-fast --source --no-check-signature base-env 2>&1 | tee log.install.${ENVNAME}.base-env\n",
      "            spack buildcache create -u local-binary base-env\n",
      "            # neptune-python-env\n",
      "            echo \"neptune-env ...\"\n",
      "            spack install --fail-fast --source --no-check-signature neptune-python-env 2>&1 | tee log.install.${ENVNAME}.neptune-env\n",
      "            spack buildcache create -u local-binary neptune-python-env\n",
      "          fi\n",
      "\n",
      "          # the rest\n",
      "          echo \"${TEMPLATE} ...\"\n",
      "          spack install --fail-fast --source --no-check-signature 2>&1 | tee log.install.${ENVNAME}.all\n",
      "          spack buildcache create -u local-binary\n",
      "\n",
      "          # Remove binary cache for next round of concretization\n",
      "          if [ \"$USE_BINARY_CACHE\" = true ] ; then\n",
      "            spack mirror rm local-binary\n",
      "          fi\n",
      "\n",
      "          # Remove buildcache config settings\n",
      "          spack config remove config:install_tree:padded_length\n",
      "\n",
      "          # Next steps: synchronize source and build cache to a central/combined mirror?\n",
      "\n",
      "          # Cleanup\n",
      "          spack clean -a\n",
      "          spack env deactivate\n",
      "\n",
      "        done\n",
      "    - name: create-env\n",
      "      run: |\n",
      "        # Set up spack-stack\n",
      "        source ./setup.sh\n",
      "        export BUILDCACHE_ENVNAME=ue-gcc-11.4.0-buildcache\n",
      "        export BUILDCACHE_ENVDIR=$PWD/envs/${BUILDCACHE_ENVNAME}\n",
      "        export ENVNAME=ue-gcc-11.4.0\n",
      "        export ENVDIR=$PWD/envs/${ENVNAME}\n",
      "        rsync -av --exclude='install' --exclude='spack.lock' --exclude='.spack_db' ${BUILDCACHE_ENVDIR}/ ${ENVDIR}/\n",
      "        spack env activate ${ENVDIR}\n",
      "\n",
      "        # Concretize and check for duplicates\n",
      "        spack concretize --force 2>&1 | tee log.concretize.${ENVNAME}\n",
      "        ${SPACK_STACK_DIR}/util/show_duplicate_packages.py -i fms -i crtm -i crtm-fix -i esmf -i mapl -i py-cython -i neptune-env\n",
      "\n",
      "        # Add binary cache back in\n",
      "        spack mirror add local-binary file://${BUILD_CACHE_PATH}/\n",
      "        echo \"Packages in combined spack build caches:\"\n",
      "        spack buildcache list\n",
      "\n",
      "        # Install from cache\n",
      "        spack install --fail-fast --source --no-check-signature 2>&1 | tee log.install.${ENVNAME}.all\n",
      "\n",
      "        # Check shared libraries\n",
      "        ${SPACK_STACK_DIR}/util/ldd_check.py $SPACK_ENV 2>&1 | tee log.ldd_check\n",
      "\n",
      "        # Create modules\n",
      "        spack clean -a\n",
      "        spack module tcl refresh -y\n",
      "        spack stack setup-meta-modules\n",
      "        spack env deactivate\n",
      "\n",
      "        # Test environment chaining\n",
      "        echo \"Test environment chaining\"\n",
      "        spack stack create env --name chaintest --template empty --site linux.default --compiler gcc --upstream ${ENVDIR}/install\n",
      "        # Retain config from upstream so we don't have to rebuild:\n",
      "        cp -r ${ENVDIR}/{site,common} $PWD/envs/chaintest/.\n",
      "        spack env activate ${PWD}/envs/chaintest\n",
      "        spack add nccmp@1.9.0.1%gcc\n",
      "        spack concretize | tee envs/chaintest/log.concretize\n",
      "        unwanted_duplicates=$(( cat envs/chaintest/log.concretize | grep -E '^ - ' | grep -Fv 'nccmp@1.9.0.1' || true ) | wc -l)\n",
      "        if [ ${unwanted_duplicates} -gt 0 ]; then echo \"Environment chaining test failed\"; exit 1; fi\n",
      "        spack env deactivate\n",
      "        echo \"Verify 'create env' warnings\"\n",
      "        echo \"# nothing\" >> ${SPACK_STACK_DIR}/envs/chaintest/site/packages.yaml\n",
      "        echo \"# nothing\" >> ${SPACK_STACK_DIR}/envs/chaintest/common/packages.yaml\n",
      "        spack stack create env --name chaintest3 --site linux.default --compiler gcc --upstream ${SPACK_STACK_DIR}/envs/chaintest/install 2>&1 | tee stderr.txt\n",
      "        cnt=$(grep -c \"WARNING.*do not match\" stderr.txt || true)\n",
      "        if [ $cnt -lt 3 ]; then echo \"Missing 'create env' warnings\"; exit 1; fi\n",
      "    - name: test-env\n",
      "      run: |\n",
      "        source /etc/profile.d/modules.sh\n",
      "        module use /home/ubuntu/spack-stack/modulefiles\n",
      "\n",
      "        export ENVNAME=ue-gcc-11.4.0\n",
      "        export ENVDIR=$PWD/envs/${ENVNAME}\n",
      "        ls -l ${ENVDIR}/install/modulefiles/Core\n",
      "\n",
      "        module use ${ENVDIR}/install/modulefiles/Core\n",
      "        module load stack-gcc/11.4.0\n",
      "        module load stack-openmpi/5.0.8\n",
      "        module available\n",
      "\n",
      "        module load jedi-ufs-env\n",
      "        module load ewok-env\n",
      "        module load soca-env\n",
      "        module list\n",
      "    ssh:\n",
      "      remoteHost: ${{inputs.resource.ip}}\n",
      "'on':\n",
      "  execute:\n",
      "    inputs:\n",
      "      resource:\n",
      "        label: Workflow Target\n",
      "        type: compute-clusters\n",
      "        autoselect: true\n",
      "        optional: false\n",
      "      gh_org:\n",
      "        label: GitHub organization/owner\n",
      "        type: string\n",
      "        default: parallelworks\n",
      "      gh_repo:\n",
      "        label: GitHub repository\n",
      "        type: string\n",
      "        default: spack-stack\n",
      "      gh_branch:\n",
      "        label: GitHub branch\n",
      "        type: string\n",
      "        default: canary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the final converted file\n",
    "with open(output_file, 'w') as stream:\n",
    "    try:\n",
    "        yaml.dump(data, stream, sort_keys=False) #, default_flow_style='True') # default_style='literal')    # Write a YAML representation of data\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "        \n",
    "print(yaml.dump(data, sort_keys=False))    # Output the document to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
